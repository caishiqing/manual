## 手撕机器学习

手撕机器学习系列模型，手推公式与numpy实现部分模型。

已实现模型全部使用minist数据集测试验证，并与sklearn做了对比：
<p align = 'center'>
<img src = 'images/accuracy.png' height = '400px'>
</p>

特殊说明：

* 所有模型训练之前都对minist数据做了LDA压缩，为了保证特征分解过程非奇异，对数据加入了少量白噪声，所以每次训练结果都有微小差异；
* 每种模型适用不同的数据维度，例如神经网络、逻辑回归适用于高维数据，而朴素贝叶斯和决策树适用于低维数据，所以每种模型使用的特征维度并不一定一样；
* 同种模型在手撕实现和sklearn之间都使用相同的参数配置，只有优化过程的细节差异。

### Requirements
适用于python2.7与python3.6，依赖包：
- numpy，实现模型结构的主要库
- scipy，稀疏矩阵与部分高级线性算法库
- tqdm，神经网络训练进度条显示

### 推断模型

#### 神经网络
numpy手撕神经网络，适用于二分类、多分类，手推梯度公式，支持dropout，使用Adam优化器，可显示训练进度条。
神经网络特性：
* 采用迭代的方式更新参数，复杂度随数据维度线性增加，所以可以适用于高维数据；
* 基于梯度更新的优化方式，所以对数值的尺度比较敏感，所以输入的规范化、参数的初始化、初始学习率的设置影响较大；
* 采用min_batch的方式迭代更新，平衡GD与SGD的优劣，dropout是一种成本很低的高效正则手段，每轮迭代随机更新部分参数。

运行示例：

	python neural_network.py --batch 200 --epochs 10 --dropout 0.5
	
<p align = 'center'>
<img src = 'images/training.png' height = '256px'>
</p>

#### 朴素贝叶斯
实现高斯分布与多项分布两种类型，Naive Bayes实现过程较为明确，没有多少优化技巧而言，所以在性能上几乎与sklearn完全一致。
朴素贝叶斯特点：
* 严格的条件独立性假设，对于低维数据表现较好，对于高维数据这样的假设容易导致欠拟合与偏差；
* 实现过程中只需要存储标签先验概率分布与似然函数，如果采用密度函数表示似然函数（如高斯分布）则记录函数参数，如果采用离散化的分布律（多项分布）则记录p(y|x)矩阵。